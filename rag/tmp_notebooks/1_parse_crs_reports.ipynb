{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8337ae0a38643d08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "# crs_folder = Path('/Users/jameslittiebrant/Data/crs_reports/reports')\n",
    "report_folder = Path('/Users/jameslittiebrant/Data/crs_reports/reports')\n",
    "file_folder = Path('/Users/jameslittiebrant/Data/crs_reports/files')\n",
    "parsed_folder = Path('/Users/jameslittiebrant/Data/crs_reports/parsed')\n",
    "parsed_folder.mkdir(exist_ok=True, parents=True)\n",
    "testing_folder = Path('testing')\n",
    "testing_folder.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "5b3e793c6d8b46a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "def hash_text_to_id(source_string: str, n_digits: int = 7) -> str:\n",
    "    hash_object = hashlib.sha256(source_string.encode())\n",
    "    hex_digest = hash_object.hexdigest()\n",
    "    hash_int = int(hex_digest, 16)\n",
    "    numeric_id = hash_int % (10**n_digits)\n",
    "    return f\"{numeric_id:0{n_digits}d}\""
   ],
   "id": "6028afd9eba8b8eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_to_markdown(element):\n",
    "    \"\"\"\n",
    "    Recursively converts a BeautifulSoup element's content to a Markdown string,\n",
    "    preserving basic formatting like bold, italics, and links.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    # Handle NavigableString (text nodes) which is a subclass of str\n",
    "    if isinstance(element, str):\n",
    "        return element\n",
    "\n",
    "    # Handle tags\n",
    "    if not hasattr(element, 'contents'):\n",
    "        return ''\n",
    "\n",
    "    for child in element.contents:\n",
    "        if isinstance(child, str):\n",
    "            text += child\n",
    "        elif child.name in ['strong', 'b']:\n",
    "            text += f\"**{convert_to_markdown(child)}**\"\n",
    "        elif child.name in ['em', 'i']:\n",
    "            text += f\"*{convert_to_markdown(child)}*\"\n",
    "        elif child.name == 'a':\n",
    "            link_text = convert_to_markdown(child).strip()\n",
    "            href = child.get('href', '')\n",
    "            text += f\"[{link_text}]({href})\"\n",
    "        elif child.name == 'span':\n",
    "            # Spans are often just for styling, so we process their content\n",
    "            text += convert_to_markdown(child)\n",
    "        else:\n",
    "            # For other unexpected tags, just get their text content\n",
    "            text += child.get_text(strip=True)\n",
    "    return text\n",
    "\n",
    "def parse_html_content(html_string):\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    structured_content = []\n",
    "    for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'table', 'ul', 'ol']):\n",
    "        if element.find_parent(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'table', 'ul', 'ol', 'li']):\n",
    "             continue\n",
    "\n",
    "        # Process based on the tag type\n",
    "        if element.name.startswith('h'):\n",
    "            structured_content.append({\n",
    "                'type': f'heading_{element.name[1]}',\n",
    "                'content': element.get_text(strip=True),\n",
    "                'id': f\"element-{hash_text_to_id(element.get_text(strip=True))}\"\n",
    "            })\n",
    "        elif element.name == 'p':\n",
    "            # Convert the paragraph content to Markdown and remove surrounding whitespace\n",
    "            markdown_content = convert_to_markdown(element).strip()\n",
    "\n",
    "            # Only add non-empty paragraphs\n",
    "            if markdown_content:\n",
    "                structured_content.append({\n",
    "                    'type': 'paragraph_markdown',\n",
    "                    'content': markdown_content,\n",
    "                'id': f\"element-{hash_text_to_id(markdown_content)}\"\n",
    "                })\n",
    "        elif element.name == 'div' and element.get_text(strip=True) and not element.find(['h1', 'h2', 'h3', 'p', 'table', 'ul', 'ol']):\n",
    "             structured_content.append({\n",
    "                'type': 'div_text',\n",
    "                'content': element.get_text(strip=True),\n",
    "                'id': f\"element-{hash_text_to_id(element.get_text(strip=True))}\"\n",
    "             })\n",
    "        elif element.name == 'table':\n",
    "            markdown_table = \"\"\n",
    "            rows = element.find_all('tr')\n",
    "            for i, row in enumerate(rows):\n",
    "                # Get all cells in the row\n",
    "                cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n",
    "                markdown_table += \"| \" + \" | \".join(cells) + \" |\\n\"\n",
    "                # Add a separator after the header row\n",
    "                if i == 0:\n",
    "                    markdown_table += \"| \" + \" | \".join(['---'] * len(cells)) + \" |\\n\"\n",
    "\n",
    "            structured_content.append({\n",
    "                'type': 'table_markdown',\n",
    "                'content': markdown_table,\n",
    "                'id': f\"element-{hash_text_to_id(markdown_table)}\"\n",
    "            })\n",
    "        elif element.name == 'ul':\n",
    "            markdown_list = \"\"\n",
    "            for item in element.find_all('li', recursive=False):\n",
    "                # Convert list item content to markdown to handle nested formatting\n",
    "                item_content = convert_to_markdown(item).strip()\n",
    "                markdown_list += f\"- {item_content}\\n\"\n",
    "\n",
    "            structured_content.append({\n",
    "                'type': 'list_markdown',\n",
    "                'content': markdown_list,\n",
    "                'id': f\"element-{hash_text_to_id(markdown_list)}\"\n",
    "            })\n",
    "        elif element.name == 'ol':\n",
    "            markdown_list = \"\"\n",
    "            for i, item in enumerate(element.find_all('li', recursive=False)):\n",
    "                 # Convert list item content to markdown to handle nested formatting\n",
    "                 item_content = convert_to_markdown(item).strip()\n",
    "                 markdown_list += f\"{i+1}. {item_content}\\n\"\n",
    "\n",
    "            structured_content.append({\n",
    "                'type': 'list_markdown',\n",
    "                'content': markdown_list,\n",
    "                'id': f\"element-{hash_text_to_id(markdown_list)}\"\n",
    "            })\n",
    "\n",
    "    return structured_content"
   ],
   "id": "59eabcd7d30f5cec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "file_folder.absolute()",
   "id": "ae78a19a3157cc1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Good\n",
    "'2025-03-18_RL31572_6c7a262e8ca1313647f6c2e2139deb25c85d49ab.html'\n",
    "\"2025-04-01_R48478_6a4277ac5f781f3f0363a6f76e64089ee5fc0041.html\"\n",
    "'2025-05-02_IF10349_8374304583e98220e31d7b89c3913fc5fdc93bef.html'\n",
    "\n",
    "# Odd\n",
    "\"2024-11-05_IN12456_e089abfacd467aa6fdad2e1c4a49fab3273496ca.html\"\n",
    "\n",
    "# simple\n",
    "'2024-11-18_LSB11249_4b1d7f66b0477bf585dbf0b394badaf425629f51.html'\n",
    "\n",
    "with open(report_folder / \"files/2025-03-18_RL31572_6c7a262e8ca1313647f6c2e2139deb25c85d49ab.html\", 'r') as file:\n",
    "    html_doc = file.read()\n",
    "parsed_data = parse_html_content(html_doc)\n",
    "\n",
    "# Print the structured data in a readable format\n",
    "import json\n",
    "print(json.dumps(parsed_data, indent=2))\n",
    "parsed_data = [x for x in parsed_data if x['content']]\n",
    "with open('test.json', 'w') as file:\n",
    "    json.dump(parsed_data, file)"
   ],
   "id": "7a2b519f644c5453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_reports = list(report_folder.glob('*.json'))",
   "id": "437934841a94daf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "report_folder.absolute()",
   "id": "fcc6d22537e2d658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_report(report):\n",
    "    formats = report['versions'][0]['formats']\n",
    "    file = [x for x in formats if x['format'] == 'HTML']\n",
    "    if file:\n",
    "        return file[0]['filename']\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_report_type(version):\n",
    "    formats = version[0]['formats']\n",
    "    file_formats = [x['format'] for x in formats]\n",
    "    if 'HTML' not in file_formats:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ],
   "id": "92f1a9734e228679",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def count_words_in_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Simple \"word\" counter by splitting on white space\n",
    "\n",
    "    Args:\n",
    "        chunk (list): A list of content items (dictionaries).\n",
    "\n",
    "    Returns:\n",
    "        int: The total word count in the chunk.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for item in chunk:\n",
    "        # Each chunk is composed of html elements\n",
    "        # Split content by whitespace to count words\n",
    "\n",
    "        count += len(item.get('content', '').split(' '))\n",
    "    return count\n",
    "\n",
    "def hierarchical_chunker_recursive(current_chunk, level, max_words, buffer):\n",
    "    \"\"\"\n",
    "    A chunk is a list of sections which are html elements such as heading, p, and table\n",
    "    buffer here is to allow small increases within a tolerance\n",
    "    \"\"\"\n",
    "    final_chunks = []\n",
    "    heading_tag = f'heading_{level}'\n",
    "\n",
    "    # 1. Find all indices of headings that will act as split points.\n",
    "    split_indices = [i for i, item in enumerate(current_chunk) if item['type'] == heading_tag]\n",
    "\n",
    "    # Base Case: If there are no headings at this level, we can't split further.\n",
    "    if not split_indices:\n",
    "        return [current_chunk]\n",
    "\n",
    "    # 2. Create all sub-groups based on the split points.\n",
    "    #    This is the streamlined logic.\n",
    "\n",
    "    # Create a list of all boundaries: start of document, all headings, and end of document.\n",
    "    all_boundaries = split_indices + [len(current_chunk)]\n",
    "    if split_indices[0] > 0:\n",
    "        all_boundaries.insert(0, 0)\n",
    "\n",
    "    # Create sub-groups by slicing between adjacent boundaries using zip.\n",
    "    sub_groups = [current_chunk[start:end] for start, end in zip(all_boundaries, all_boundaries[1:])]\n",
    "\n",
    "    # 3. Process each sub-group.\n",
    "    for group in sub_groups:\n",
    "        if not group:\n",
    "            continue\n",
    "\n",
    "        word_count = count_words_in_chunk(group)\n",
    "        if word_count <= max_words + buffer:\n",
    "            # If the group is small enough, it's a final chunk.\n",
    "            final_chunks.append(group)\n",
    "        else:\n",
    "            # If the group is too large, recurse deeper with a step up in the level to identify any sub-sections\n",
    "            deeper_chunks = hierarchical_chunker_recursive(group, level + 1, max_words, buffer)\n",
    "            final_chunks.extend(deeper_chunks)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def chunk_document(data, max_words, buffer, target_level=1):\n",
    "    \"\"\"\n",
    "    This processes the entire document if needed.\n",
    "    \"\"\"\n",
    "    return hierarchical_chunker_recursive(data, target_level, max_words, buffer)\n"
   ],
   "id": "83a0a59f090ae3e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Chunk Merging Logic ---\n",
    "def merge_chunks(chunks, target_word_count, last_section_buffer=200):\n",
    "    \"\"\"\n",
    "    Merges smaller chunks together to reach a target word count, with special\n",
    "    handling for small trailing chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): A list of chunks, where each chunk is a list of content items.\n",
    "        target_word_count (int): The desired word count for the merged chunks.\n",
    "        last_section_buffer (int): If the final chunk is smaller than this word count,\n",
    "                                   it will be merged with the previous chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A new list of merged chunks.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged_chunks = []\n",
    "    # Start with the first chunk\n",
    "    current_merged_chunk = list(chunks[0])\n",
    "    current_word_count = count_words_in_chunk(current_merged_chunk)\n",
    "\n",
    "    # Iterate through the rest of the chunks\n",
    "    for next_chunk in chunks[1:]:\n",
    "        next_chunk_word_count = count_words_in_chunk(next_chunk)\n",
    "\n",
    "        # If adding the next chunk doesn't exceed the target, merge it\n",
    "        if current_word_count + next_chunk_word_count <= target_word_count:\n",
    "            current_merged_chunk.extend(next_chunk)\n",
    "            current_word_count += next_chunk_word_count\n",
    "        else:\n",
    "            # Otherwise, finalize the current merged chunk\n",
    "            merged_chunks.append(current_merged_chunk)\n",
    "            # And start a new one\n",
    "            current_merged_chunk = list(next_chunk)\n",
    "            current_word_count = next_chunk_word_count\n",
    "\n",
    "    # Don't forget to add the last processed chunk\n",
    "    if current_merged_chunk:\n",
    "        merged_chunks.append(current_merged_chunk)\n",
    "\n",
    "    # --- NEW: Handle small trailing chunks ---\n",
    "    # If there are at least two chunks and the last one is too small, merge it back.\n",
    "    if len(merged_chunks) >= 2 and count_words_in_chunk(merged_chunks[-1]) < last_section_buffer:\n",
    "        # Extend the second-to-last chunk with the contents of the last one\n",
    "        merged_chunks[-2].extend(merged_chunks[-1])\n",
    "        # Remove the now-absorbed last chunk\n",
    "        merged_chunks.pop()\n",
    "\n",
    "    return merged_chunks"
   ],
   "id": "d125e374efda9930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Citation Logic ---\n",
    "\n",
    "def add_citation_positional_references(chunks):\n",
    "    \"\"\"\n",
    "    For each chunk, the chunk has a position in the whole document\n",
    "    For each element within that chunk, it has a positional value\n",
    "    We want to create a mapping between the elements and the chunk for future use\n",
    "    This future use can use the chunk id as a main citation, or it can break down\n",
    "    The existing chunk into pin-point citations for the sections\n",
    "    We will also introduce the absolute position of a section in the entire document\n",
    "    This gives a large structure to exploit where a section can be located by its chunk\n",
    "    Or a section element can be located by its absolute document position\n",
    "    Args:\n",
    "        chunks (list): A list of chunks. Which are lists of section elements\n",
    "    Returns:\n",
    "        list: The list of chunks with all the positional markers added\n",
    "    \"\"\"\n",
    "    cited_chunks = []\n",
    "    document_position = 1\n",
    "    # Enumerate from 1 to get human-readable chunk IDs\n",
    "    for chunk_id, chunk in enumerate(chunks, 1):\n",
    "        new_chunk = []\n",
    "        # Enumerate from 1 for passage IDs within the chunk\n",
    "        for passage_id, passage in enumerate(chunk, 1):\n",
    "            # Add a citation to the passage level for the chunk\n",
    "            # This tags the chunk_id with the section passage id\n",
    "            # Create a copy to avoid modifying the original data in place.\n",
    "            new_passage = passage.copy()\n",
    "            new_passage['chunk_position'] = chunk_id\n",
    "            new_passage['intra_chunk_position'] = passage_id\n",
    "            new_passage['document_position'] = document_position\n",
    "            document_position += 1\n",
    "            new_chunk.append(new_passage)\n",
    "        cited_chunks.append(new_chunk)\n",
    "    return cited_chunks\n",
    "\n",
    "def find_highest_heading(passages, min_necessary=1):\n",
    "    # Since not all documents will have the main heading at # in markdown\n",
    "    # We want to adapt dynamically to the largest heading (which is the min(h1, h2...) html element\n",
    "    passage_counts = Counter([x['type'] for x in passages])\n",
    "    # min_necessary is set to greater than 1 to make sure that our splits aren't on a singular section heading\n",
    "    possible_headings = [passage for passage, _count in passage_counts.items() if _count > min_necessary]\n",
    "    possible_headings = [x for x in possible_headings if 'heading' in x]\n",
    "    possible_headings = [int(x.split('_')[1]) for x in possible_headings]\n",
    "    if len(possible_headings) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(possible_headings)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    JSON_FILE_PATH = 'test.json'\n",
    "    BASE_CITATION = \"RL31572\"\n",
    "    # Config for the initial splitting phase\n",
    "    INITIAL_MAX_WORDS = 500\n",
    "    INITIAL_BUFFER = 250\n",
    "    MINIMUM_NECESSARY_HEADINGS = 1\n",
    "    # Config for the merging phase\n",
    "    # This can be set to be higher if the actual retrieval text is desired to be bigger\n",
    "    # This can help offset small chunks with context around it\n",
    "    TARGET_MERGE_WORD_COUNT = 500\n",
    "    TARGET_MERGE_BUFFER = 250\n",
    "\n",
    "    try:\n",
    "        # Load the document from the JSON file\n",
    "        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            document_data_raw = json.load(f)\n",
    "\n",
    "        # Add original document index to each passage at the beginning\n",
    "        document_data = []\n",
    "        for i, passage in enumerate(document_data_raw):\n",
    "            passage['document_citation'] = BASE_CITATION\n",
    "            document_data.append(passage)\n",
    "\n",
    "        # --- Step 1: Perform the initial hierarchical chunking ---\n",
    "        highest_heading = find_highest_heading(document_data, min_necessary=MINIMUM_NECESSARY_HEADINGS)\n",
    "        initial_chunks = chunk_document(document_data, INITIAL_MAX_WORDS, INITIAL_BUFFER, target_level=highest_heading)\n",
    "        print(f\"Step 1: Document initially split into {len(initial_chunks)} chunks.\\n\")\n",
    "\n",
    "        # --- Step 2: Perform the merging post-processing ---\n",
    "        final_merged_chunks = merge_chunks(initial_chunks, TARGET_MERGE_WORD_COUNT, TARGET_MERGE_BUFFER)\n",
    "        print(\"---\" * 15)\n",
    "        print(f\"\\nStep 2: Merged into {len(final_merged_chunks)} final chunks.\")\n",
    "        print(f\"Target Merge Size: {TARGET_MERGE_WORD_COUNT} words\\n\")\n",
    "\n",
    "        # --- Step 3: Add citations to the final chunks ---\n",
    "        final_cited_chunks = add_citation_positional_references(final_merged_chunks)\n",
    "        print(\"---\" * 15)\n",
    "        print(f\"\\nStep 3: Added citations to {len(final_cited_chunks)} chunks.\")\n",
    "        print(f\"Base Citation: '{BASE_CITATION}'\\n\")\n",
    "\n",
    "        # --- Output and Verification ---\n",
    "        print(\"---\" * 15)\n",
    "        print(\"\\nFinal Output Verification:\\n\")\n",
    "        total_word_count = 0\n",
    "        for i, chunk in enumerate(final_cited_chunks):\n",
    "            word_count = count_words_in_chunk(chunk)\n",
    "            total_word_count += word_count\n",
    "\n",
    "            first_item_type = chunk[0].get('type', 'N/A')\n",
    "            first_item_content = chunk[0].get('content', '').replace('\\n', ' ')[:70]\n",
    "            first_item_citation = chunk[0].get('citation', 'N/A')\n",
    "            first_item_doc_index = chunk[0].get('doc_index', -1)\n",
    "\n",
    "            print(f\"--- Final Chunk {i+1} ---\")\n",
    "            print(f\"  Word Count: {word_count}\")\n",
    "            print(f\"  Items: {len(chunk)}\")\n",
    "            print(f\"  Starts with '{first_item_type}': \\\"{first_item_content}...\\\"\")\n",
    "            print(f\"  First Citation: {first_item_citation}\")\n",
    "            print(f\"  First Doc Index: {first_item_doc_index}\")\n",
    "            print()\n",
    "\n",
    "        print(f\"Total word count across all final chunks: {total_word_count}\")\n",
    "        print(f\"Total word count in original document: {count_words_in_chunk(document_data)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{JSON_FILE_PATH}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from the file '{JSON_FILE_PATH}'.\")\n"
   ],
   "id": "fac1791646905c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_cited_chunks[-2]",
   "id": "bb5618debbce1a4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_folder = Path('parsed')\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "errors_folder = Path('/Users/jameslittiebrant/Data/crs_reports/errors')\n",
    "errors_folder.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "f55afadfc88dd792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "json_files = Path('/Users/jameslittiebrant/Data/crs_reports/reports/reports')\n",
    "json_files = [x for x in json_files.glob('*.json')]"
   ],
   "id": "4c335a091fe14c23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ],
   "id": "dbd3a709f0c8bd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_report_metadata(metadata):\n",
    "    parsed_metadata = dict()\n",
    "    parsed_metadata['id'] = metadata['id']\n",
    "    parsed_metadata['type'] = metadata['type']\n",
    "    parsed_metadata['typeId'] = metadata['typeId']\n",
    "    parsed_metadata['number'] = metadata['number']\n",
    "    parsed_metadata['active'] = metadata['active']\n",
    "    parsed_metadata['source'] = metadata['source']\n",
    "    parsed_metadata['topics'] = metadata['topics']\n",
    "    _version_info = metadata['versions'][0]\n",
    "    parsed_metadata['version_id'] = _version_info['id']\n",
    "    parsed_metadata['date'] = _version_info['date']\n",
    "    parsed_metadata['retrieved_date'] = _version_info['retrieved']\n",
    "    parsed_metadata['title'] = _version_info['title']\n",
    "    parsed_metadata['summary'] = _version_info['summary']\n",
    "    parsed_metadata['source_file'] = [x for x in _version_info['formats'] if x['format'] == 'HTML'][0]['filename']\n",
    "    return parsed_metadata"
   ],
   "id": "39c28b9cb0ff7986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parse_report_metadata(data)",
   "id": "766957857099c362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_file(json_file, source_folder):\n",
    "    with open(source_folder.joinpath(f\"{json_file}\"), 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    try:\n",
    "        filename = [x for x in data['versions'][0]['formats'] if x['format'] == 'HTML'][0]['filename']\n",
    "    except IndexError as e:\n",
    "        return False, json_file.name, ''\n",
    "\n",
    "    with open(source_folder.joinpath(f'{filename}'), 'r') as f:\n",
    "        doc = f.read()\n",
    "    return True, doc, data"
   ],
   "id": "3846e5f8b772f4be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I'm targeting around 500 words because quite often these can run over that\n",
    "# So this is more of a baseline for getting the initial chunk before merging it where there might be about a doubling\n",
    "# For instance, an initial max word of 500 + 250 buffer = 750\n",
    "# If there is one chunk at 400, and the next at 750, then we're at 1150 words. If it's an end, then + 250 = 1400 words.\n",
    "# This provides some level of variability, but it's at a level that is closer to a meaningful segment of a document at the expense of variability\n",
    "INITIAL_MAX_WORDS = 500\n",
    "INITIAL_BUFFER = 250\n",
    "MINIMUM_NECESSARY_HEADINGS = 1\n",
    "TARGET_MERGE_WORD_COUNT = 500\n",
    "TARGET_MERGE_BUFFER = 250"
   ],
   "id": "8e4e8511b90eaed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def format_chunks(chunk, join_separator='\\n\\n'):\n",
    "    chunk_text = join_separator.join([x['content'].strip() for x in chunk])\n",
    "    chunk_type = chunk[0].get('type','')\n",
    "    document_citation = chunk[0].get('document_citation')\n",
    "    chunk_position = chunk[0].get('chunk_position')\n",
    "    element_ids = [x.get('id') for x in chunk]\n",
    "    chunk_section_start = chunk[0].get('document_position')\n",
    "    chunk_section_end = chunk[-1].get('document_position')\n",
    "    chunk_element = {\n",
    "        'content':chunk_text,\n",
    "        'type':chunk_type,\n",
    "        'document_citation':document_citation,\n",
    "        'chunk_position':chunk_position,\n",
    "        'element_ids':element_ids,\n",
    "        'chunk_start':chunk_section_start,\n",
    "        'chunk_end':chunk_section_end\n",
    "    }\n",
    "    return chunk_element"
   ],
   "id": "7357920aa08698ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_cited_chunks[0]",
   "id": "dbeade0fa699bc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "errored_files = list()\n",
    "re_parse_files = list()\n",
    "parsed_reports_metadata = list()\n",
    "parsed_sections = list()\n",
    "parsed_chunks = list()"
   ],
   "id": "288c0bbc2e3d77e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_point = len(errored_files) + len(parse_report_metadata(data))\n",
    "for json_file in tqdm(json_files[start_point:]):\n",
    "    did_load, html_document, metadata = load_file(json_file, report_folder)\n",
    "    if not did_load:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            _error_data = json.load(f)\n",
    "        errored_files.append({'filename': json_file.name, 'json':_error_data, 'error':'error_loading'})\n",
    "        continue\n",
    "\n",
    "    if len(html_document.strip()) == 0:\n",
    "        print('no html')\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            _error_data = json.load(f)\n",
    "        errored_files.append({'filename': json_file.name, 'json':_error_data, 'error':'no_html'})\n",
    "        continue\n",
    "\n",
    "    metadata = parse_report_metadata(metadata)\n",
    "    document_data_raw = parse_html_content(html_document)\n",
    "    if len(document_data_raw) == 0:\n",
    "        errored_files.append({'filename': json_file.name, 'json':metadata, 'error':'html_did_not_parse'})\n",
    "        continue\n",
    "    document_data = []\n",
    "    for i, passage in enumerate(document_data_raw):\n",
    "        passage['document_citation'] = metadata['id']\n",
    "        document_data.append(passage)\n",
    "\n",
    "    highest_heading = find_highest_heading(document_data, min_necessary=MINIMUM_NECESSARY_HEADINGS)\n",
    "    initial_chunks = chunk_document(document_data, INITIAL_MAX_WORDS, INITIAL_BUFFER, target_level=highest_heading)\n",
    "    merged_chunks = merge_chunks(initial_chunks, TARGET_MERGE_WORD_COUNT, TARGET_MERGE_BUFFER)\n",
    "    cited_chunks = add_citation_positional_references(merged_chunks)\n",
    "    formatted_chunks = [format_chunks(chunk, '\\n\\n') for chunk in cited_chunks]\n",
    "\n",
    "    parsed_reports_metadata.append(metadata)\n",
    "    parsed_chunks.extend(formatted_chunks)\n",
    "    for chunk in cited_chunks:\n",
    "        parsed_sections.extend(chunk)"
   ],
   "id": "dded6a8d3dc6c993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(parsed_sections), len(parsed_chunks), len(parsed_reports_metadata)",
   "id": "cf986200d4921d56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _chunk in parsed_chunks:\n",
    "    _hash_string = f\"{_chunk['chunk_start']}_{_chunk['chunk_end']}_{_chunk['content']}\"\n",
    "    _chunk['id'] = f\"chunk-{hash_text_to_id(_hash_string, n_digits=10)}\"\n",
    "    _chunk['document_id'] = _chunk['document_citation']"
   ],
   "id": "a8c329f21f37ab85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for section in parsed_sections:\n",
    "    section['document_id'] = section['document_citation']"
   ],
   "id": "57dd3c8cd8465761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _report_metadata in parsed_reports_metadata:\n",
    "    _report_metadata['type_id'] = _report_metadata['typeId']\n",
    "    del _report_metadata['typeId']"
   ],
   "id": "44a0be285dd53dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _report_metadata in parsed_reports_metadata:\n",
    "    _report_metadata['version_id'] = str(_report_metadata['version_id'])"
   ],
   "id": "f681d751d10cedd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "run_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "run_time = '20250723_1319'\n",
    "with open(parsed_folder.joinpath(f'{run_time}_sections.json'), 'w') as f:\n",
    "    json.dump(parsed_sections, f)\n",
    "with open(parsed_folder.joinpath(f'{run_time}_chunks.json'), 'w') as f:\n",
    "    json.dump(parsed_chunks, f)\n",
    "with open(parsed_folder.joinpath(f'{run_time}_files.json'), 'w') as f:\n",
    "    json.dump(parsed_reports_metadata, f)\n",
    "with open(parsed_folder.joinpath(f'{run_time}_errors.json'), 'w') as f:\n",
    "    json.dump(errored_files, f)"
   ],
   "id": "78e16b86d18d1c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(parsed_folder.joinpath(f'{run_time}_files.json'), 'w') as f:\n",
    "    json.dump(parsed_reports_metadata, f)"
   ],
   "id": "23a29163c7994ede",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for key, value in parsed_sections[0].items():\n",
    "    if isinstance(value, str):\n",
    "        value_str = 'str'\n",
    "    elif isinstance(value, int):\n",
    "        value_str = 'int'\n",
    "    elif isinstance(value, list):\n",
    "        value_str = 'List[str]'\n",
    "    elif isinstance(value, float):\n",
    "        value_str = 'float'\n",
    "    else:\n",
    "        value_str = str(type(value))\n",
    "    print(f\"{key}: {value_str}\")"
   ],
   "id": "98ee7ae60e036024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64e1398a36ad9a64",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
