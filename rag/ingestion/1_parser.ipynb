{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "report_folder = Path('reports/reports')\n",
    "file_folder = Path('reports/files')"
   ],
   "id": "5b3e793c6d8b46a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_to_markdown(element):\n",
    "    \"\"\"\n",
    "    Recursively converts a BeautifulSoup element's content to a Markdown string,\n",
    "    preserving basic formatting like bold, italics, and links.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    # Handle NavigableString (text nodes) which is a subclass of str\n",
    "    if isinstance(element, str):\n",
    "        return element\n",
    "\n",
    "    # Handle tags\n",
    "    if not hasattr(element, 'contents'):\n",
    "        return ''\n",
    "\n",
    "    for child in element.contents:\n",
    "        if isinstance(child, str):\n",
    "            text += child\n",
    "        elif child.name in ['strong', 'b']:\n",
    "            text += f\"**{convert_to_markdown(child)}**\"\n",
    "        elif child.name in ['em', 'i']:\n",
    "            text += f\"*{convert_to_markdown(child)}*\"\n",
    "        elif child.name == 'a':\n",
    "            link_text = convert_to_markdown(child).strip()\n",
    "            href = child.get('href', '')\n",
    "            text += f\"[{link_text}]({href})\"\n",
    "        elif child.name == 'span':\n",
    "            # Spans are often just for styling, so we process their content\n",
    "            text += convert_to_markdown(child)\n",
    "        else:\n",
    "            # For other unexpected tags, just get their text content\n",
    "            text += child.get_text(strip=True)\n",
    "    return text\n",
    "\n",
    "def parse_html_content(html_string):\n",
    "    \"\"\"\n",
    "    Parses an HTML string to extract structural elements like headings, paragraphs, tables, and lists.\n",
    "\n",
    "    This function uses BeautifulSoup to parse the HTML, ignoring images and scripts.\n",
    "    It identifies headings, paragraphs, tables, and lists (ul, ol),\n",
    "    and returns a structured list of these elements. Paragraphs, tables, and lists are converted\n",
    "    to Markdown format.\n",
    "\n",
    "    Args:\n",
    "        html_string (str): A string containing the HTML content to be parsed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a\n",
    "              structural element found in the HTML. Each element has a 'type'\n",
    "              and 'content'.\n",
    "    \"\"\"\n",
    "    # Initialize BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "    # This list will hold our structured data\n",
    "    structured_content = []\n",
    "\n",
    "    # Find all relevant tags that define the structure of the document\n",
    "    for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'table', 'ul', 'ol']):\n",
    "        # Skip elements that are parents of other processed elements to avoid duplication\n",
    "        if element.find_parent(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'table', 'ul', 'ol', 'li']):\n",
    "             continue\n",
    "\n",
    "        # Process based on the tag type\n",
    "        if element.name.startswith('h'):\n",
    "            structured_content.append({\n",
    "                'type': f'heading_{element.name[1]}',\n",
    "                'content': element.get_text(strip=True)\n",
    "            })\n",
    "        elif element.name == 'p':\n",
    "            # Convert the paragraph content to Markdown and remove surrounding whitespace\n",
    "            markdown_content = convert_to_markdown(element).strip()\n",
    "\n",
    "            # Only add non-empty paragraphs\n",
    "            if markdown_content:\n",
    "                structured_content.append({\n",
    "                    'type': 'paragraph_markdown',\n",
    "                    'content': markdown_content\n",
    "                })\n",
    "        elif element.name == 'div' and element.get_text(strip=True) and not element.find(['h1', 'h2', 'h3', 'p', 'table', 'ul', 'ol']):\n",
    "             structured_content.append({\n",
    "                'type': 'div_text',\n",
    "                'content': element.get_text(strip=True)\n",
    "             })\n",
    "        elif element.name == 'table':\n",
    "            markdown_table = \"\"\n",
    "            rows = element.find_all('tr')\n",
    "            for i, row in enumerate(rows):\n",
    "                # Get all cells in the row\n",
    "                cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n",
    "                markdown_table += \"| \" + \" | \".join(cells) + \" |\\n\"\n",
    "                # Add a separator after the header row\n",
    "                if i == 0:\n",
    "                    markdown_table += \"| \" + \" | \".join(['---'] * len(cells)) + \" |\\n\"\n",
    "\n",
    "            structured_content.append({\n",
    "                'type': 'table_markdown',\n",
    "                'content': markdown_table\n",
    "            })\n",
    "        elif element.name == 'ul':\n",
    "            markdown_list = \"\"\n",
    "            for item in element.find_all('li', recursive=False):\n",
    "                # Convert list item content to markdown to handle nested formatting\n",
    "                item_content = convert_to_markdown(item).strip()\n",
    "                markdown_list += f\"- {item_content}\\n\"\n",
    "\n",
    "            structured_content.append({\n",
    "                'type': 'list_markdown',\n",
    "                'content': markdown_list\n",
    "            })\n",
    "        elif element.name == 'ol':\n",
    "            markdown_list = \"\"\n",
    "            for i, item in enumerate(element.find_all('li', recursive=False)):\n",
    "                 # Convert list item content to markdown to handle nested formatting\n",
    "                 item_content = convert_to_markdown(item).strip()\n",
    "                 markdown_list += f\"{i+1}. {item_content}\\n\"\n",
    "\n",
    "            structured_content.append({\n",
    "                'type': 'list_markdown',\n",
    "                'content': markdown_list\n",
    "            })\n",
    "\n",
    "\n",
    "    return structured_content\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to read the HTML file, parse it, and print the result.\n",
    "    \"\"\"\n",
    "    # Example HTML content with formatted text to demonstrate markdown conversion.\n",
    "    html_doc = \"\"\"\n",
    "    <div>\n",
    "        <h2>Example with Formatting</h2>\n",
    "        <p>This paragraph has <strong>bold text</strong> and <em>italic text</em>.</p>\n",
    "        <p>It also has a <a href=\"https://example.com\">link with <i>italic text inside</i></a>.</p>\n",
    "        <p>Here is another paragraph with <b>bold</b> and <i>italics</i>.</p>\n",
    "        <ul>\n",
    "            <li>List item with <strong>bold</strong></li>\n",
    "            <li>Another list item with <em>italics</em></li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the HTML\n",
    "    parsed_data = parse_html_content(html_doc)\n",
    "\n",
    "    # Print the structured data in a readable format\n",
    "    import json\n",
    "    print(json.dumps(parsed_data, indent=2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "id": "59eabcd7d30f5cec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Good\n",
    "'2025-03-18_RL31572_6c7a262e8ca1313647f6c2e2139deb25c85d49ab.html'\n",
    "\"2025-04-01_R48478_6a4277ac5f781f3f0363a6f76e64089ee5fc0041.html\"\n",
    "'2025-05-02_IF10349_8374304583e98220e31d7b89c3913fc5fdc93bef.html'\n",
    "\n",
    "# Odd\n",
    "\"2024-11-05_IN12456_e089abfacd467aa6fdad2e1c4a49fab3273496ca.html\"\n",
    "\n",
    "# simple\n",
    "'2024-11-18_LSB11249_4b1d7f66b0477bf585dbf0b394badaf425629f51.html'\n",
    "\n",
    "with open(file_folder / \"2024-11-05_IN12456_e089abfacd467aa6fdad2e1c4a49fab3273496ca.html\", 'r') as file:\n",
    "    html_doc = file.read()\n",
    "parsed_data = parse_html_content(html_doc)\n",
    "\n",
    "# Print the structured data in a readable format\n",
    "import json\n",
    "print(json.dumps(parsed_data, indent=2))\n",
    "parsed_data = [x for x in parsed_data if x['content']]\n",
    "with open('test.json', 'w') as file:\n",
    "    json.dump(parsed_data, file)"
   ],
   "id": "7a2b519f644c5453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def count_words_in_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Calculates the total number of words in a chunk of content.\n",
    "    A chunk is a list of dictionaries, where each dictionary has a 'content' key.\n",
    "\n",
    "    Args:\n",
    "        chunk (list): A list of content items (dictionaries).\n",
    "\n",
    "    Returns:\n",
    "        int: The total word count in the chunk.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for item in chunk:\n",
    "        # Split content by whitespace to count words\n",
    "        count += len(item.get('content', '').split())\n",
    "    return count\n",
    "\n",
    "# --- Original Hierarchical Chunker Logic (from the first script) ---\n",
    "# This part is included to make the example runnable. In a real pipeline,\n",
    "# you would import this function or run the scripts sequentially.\n",
    "\n",
    "def hierarchical_chunker_recursive(current_chunk, level, max_words, buffer):\n",
    "    \"\"\"\n",
    "    Recursively splits a chunk of a document into smaller chunks based on headings.\n",
    "    \"\"\"\n",
    "    final_chunks = []\n",
    "    heading_tag = f'heading_{level}'\n",
    "    has_headings_at_this_level = any(item['type'] == heading_tag for item in current_chunk)\n",
    "\n",
    "    if not has_headings_at_this_level:\n",
    "        return [current_chunk]\n",
    "\n",
    "    sub_groups = []\n",
    "    current_group = []\n",
    "    first_heading_index = -1\n",
    "    for i, item in enumerate(current_chunk):\n",
    "        if item['type'] == heading_tag:\n",
    "            first_heading_index = i\n",
    "            break\n",
    "\n",
    "    if first_heading_index > 0:\n",
    "        initial_group = current_chunk[:first_heading_index]\n",
    "        word_count = count_words_in_chunk(initial_group)\n",
    "        if word_count > max_words + buffer:\n",
    "            deeper_chunks = hierarchical_chunker_recursive(initial_group, level + 1, max_words, buffer)\n",
    "            final_chunks.extend(deeper_chunks)\n",
    "        else:\n",
    "            final_chunks.append(initial_group)\n",
    "        remaining_items = current_chunk[first_heading_index:]\n",
    "    else:\n",
    "        remaining_items = current_chunk\n",
    "\n",
    "    for item in remaining_items:\n",
    "        if item['type'] == heading_tag:\n",
    "            if current_group:\n",
    "                sub_groups.append(current_group)\n",
    "            current_group = [item]\n",
    "        else:\n",
    "            current_group.append(item)\n",
    "\n",
    "    if current_group:\n",
    "        sub_groups.append(current_group)\n",
    "\n",
    "    for group in sub_groups:\n",
    "        word_count = count_words_in_chunk(group)\n",
    "        if word_count <= max_words + buffer:\n",
    "            final_chunks.append(group)\n",
    "        else:\n",
    "            deeper_chunks = hierarchical_chunker_recursive(group, level + 1, max_words, buffer)\n",
    "            final_chunks.extend(deeper_chunks)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def chunk_document(data, max_words, buffer, target_level=1):\n",
    "    \"\"\"\n",
    "    Main function to start the hierarchical chunking process.\n",
    "    \"\"\"\n",
    "    return hierarchical_chunker_recursive(data, 3, max_words, buffer)\n",
    "\n",
    "\n",
    "# --- Chunk Merging Logic ---\n",
    "\n",
    "def merge_chunks(chunks, target_word_count):\n",
    "    \"\"\"\n",
    "    Merges smaller chunks together to reach a target word count.\n",
    "\n",
    "    This function iterates through a list of chunks and combines adjacent ones\n",
    "    as long as the combined word count does not exceed the target.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): A list of chunks, where each chunk is a list of content items.\n",
    "        target_word_count (int): The desired word count for the merged chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A new list of merged chunks.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged_chunks = []\n",
    "    # Start with the first chunk\n",
    "    current_merged_chunk = list(chunks[0])\n",
    "    current_word_count = count_words_in_chunk(current_merged_chunk)\n",
    "\n",
    "    # Iterate through the rest of the chunks\n",
    "    for next_chunk in chunks[1:]:\n",
    "        next_chunk_word_count = count_words_in_chunk(next_chunk)\n",
    "\n",
    "        # If adding the next chunk doesn't exceed the target, merge it\n",
    "        if current_word_count + next_chunk_word_count <= target_word_count:\n",
    "            current_merged_chunk.extend(next_chunk)\n",
    "            current_word_count += next_chunk_word_count\n",
    "        else:\n",
    "            # Otherwise, finalize the current merged chunk\n",
    "            merged_chunks.append(current_merged_chunk)\n",
    "            # And start a new one with the next chunk\n",
    "            current_merged_chunk = list(next_chunk)\n",
    "            current_word_count = next_chunk_word_count\n",
    "\n",
    "    # Don't forget to add the last processed chunk\n",
    "    if current_merged_chunk:\n",
    "        merged_chunks.append(current_merged_chunk)\n",
    "\n",
    "    return merged_chunks\n",
    "\n",
    "\n",
    "# --- Citation Logic ---\n",
    "\n",
    "def add_citations(chunks, base_citation):\n",
    "    \"\"\"\n",
    "    Adds a unique citation to each passage within each chunk.\n",
    "\n",
    "    The function iterates through each chunk and each passage, adding a\n",
    "    'citation' field in the format of \"[base_citation_chunk_id__passage_id]\".\n",
    "\n",
    "    Args:\n",
    "        chunks (list): A list of chunks.\n",
    "        base_citation (str): The base string to use for citations.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of chunks with citations added to each passage.\n",
    "    \"\"\"\n",
    "    cited_chunks = []\n",
    "    # Enumerate from 1 to get human-readable chunk IDs\n",
    "    for chunk_id, chunk in enumerate(chunks, 1):\n",
    "        new_chunk = []\n",
    "        # Enumerate from 1 for passage IDs within the chunk\n",
    "        for passage_id, passage in enumerate(chunk, 0):\n",
    "            # Create a copy to avoid modifying the original data in place.\n",
    "            # This preserves all existing fields, including 'doc_index'.\n",
    "            new_passage = passage.copy()\n",
    "            new_passage['citation'] = f\"{base_citation}_{chunk_id}__{passage_id}\"\n",
    "            new_chunk.append(new_passage)\n",
    "        cited_chunks.append(new_chunk)\n",
    "    return cited_chunks\n",
    "\n",
    "def find_highest_heading(passages, min_necessary=1):\n",
    "    passage_counts = Counter([x['type'] for x in passages])\n",
    "    possible_headings = [passage for passage, _count in passage_counts.items() if _count > min_necessary]\n",
    "    possible_headings = [x for x in possible_headings if 'heading' in x]\n",
    "    possible_headings = [int(x.split('_')[1]) for x in possible_headings]\n",
    "    return min(possible_headings)\n",
    "\n",
    "def skip_heading_1(passages, min_necessary=1):\n",
    "    passage_counts = Counter([x['type'] for x in passages])\n",
    "    possible_headings = {passage_type:_count for passage_type, _count in passage_counts.items() if 'heading' in passage_type}\n",
    "    if len(possible_headings) == 0:\n",
    "        return 0\n",
    "    if possible_headings.get('heading_1', 0):\n",
    "        if possible_headings.get('heading_1') > min_necessary:\n",
    "            return 1\n",
    "        else:\n",
    "            _min_header = min([int(passage_type.split('_')[1]) for passage_type in possible_headings.keys()])\n",
    "            return _min_header\n",
    "    return -1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    JSON_FILE_PATH = 'test.json'\n",
    "    BASE_CITATION = \"RL31572\"\n",
    "    # Config for the initial splitting phase\n",
    "    INITIAL_MAX_WORDS = 400\n",
    "    INITIAL_BUFFER = 400\n",
    "    MINIMUM_NECESSARY_HEADINGS = 1\n",
    "    # Config for the merging phase\n",
    "    TARGET_MERGE_WORD_COUNT = 1000\n",
    "\n",
    "    try:\n",
    "        # Load the document from the JSON file\n",
    "        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            document_data_raw = json.load(f)\n",
    "\n",
    "        # Add original document index to each passage at the beginning\n",
    "        document_data = []\n",
    "        for i, passage in enumerate(document_data_raw):\n",
    "            passage['doc_index'] = i\n",
    "            document_data.append(passage)\n",
    "\n",
    "        # --- Step 1: Perform the initial hierarchical chunking ---\n",
    "        highest_heading = skip_heading_1(document_data, min_necessary=MINIMUM_NECESSARY_HEADINGS)\n",
    "        initial_chunks = chunk_document(document_data, INITIAL_MAX_WORDS, INITIAL_BUFFER, target_level=highest_heading)\n",
    "        print(f\"Step 1: Document initially split into {len(initial_chunks)} chunks.\\n\")\n",
    "\n",
    "        # --- Step 2: Perform the merging post-processing ---\n",
    "        final_merged_chunks = merge_chunks(initial_chunks, TARGET_MERGE_WORD_COUNT)\n",
    "        print(\"---\" * 15)\n",
    "        print(f\"\\nStep 2: Merged into {len(final_merged_chunks)} final chunks.\")\n",
    "        print(f\"Target Merge Size: {TARGET_MERGE_WORD_COUNT} words\\n\")\n",
    "\n",
    "        # --- Step 3: Add citations to the final chunks ---\n",
    "        final_cited_chunks = add_citations(final_merged_chunks, BASE_CITATION)\n",
    "        print(\"---\" * 15)\n",
    "        print(f\"\\nStep 3: Added citations to {len(final_cited_chunks)} chunks.\")\n",
    "        print(f\"Base Citation: '{BASE_CITATION}'\\n\")\n",
    "\n",
    "        # --- Output and Verification ---\n",
    "        print(\"---\" * 15)\n",
    "        print(\"\\nFinal Output Verification:\\n\")\n",
    "        total_word_count = 0\n",
    "        for i, chunk in enumerate(final_cited_chunks):\n",
    "            word_count = count_words_in_chunk(chunk)\n",
    "            total_word_count += word_count\n",
    "\n",
    "            first_item_type = chunk[0].get('type', 'N/A')\n",
    "            first_item_content = chunk[0].get('content', '').replace('\\n', ' ')[:70]\n",
    "            first_item_citation = chunk[0].get('citation', 'N/A')\n",
    "            first_item_doc_index = chunk[0].get('doc_index', -1)\n",
    "\n",
    "            print(f\"--- Final Chunk {i+1} ---\")\n",
    "            print(f\"  Word Count: {word_count}\")\n",
    "            print(f\"  Items: {len(chunk)}\")\n",
    "            print(f\"  Starts with '{first_item_type}': \\\"{first_item_content}...\\\"\")\n",
    "            print(f\"  First Citation: {first_item_citation}\")\n",
    "            print(f\"  First Doc Index: {first_item_doc_index}\")\n",
    "            print()\n",
    "\n",
    "        print(f\"Total word count across all final chunks: {total_word_count}\")\n",
    "        print(f\"Total word count in original document: {count_words_in_chunk(document_data)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{JSON_FILE_PATH}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from the file '{JSON_FILE_PATH}'.\")\n"
   ],
   "id": "fac1791646905c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_cited_chunks[0]",
   "id": "de9ae1d43bdd5d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def format_chunks(chunks):\n",
    "    markdown_text = list()\n",
    "    for chunk in chunks:\n",
    "        if 'heading' in chunk['type']:\n",
    "            heading_strength = int(chunk['type'].split('_')[1])\n",
    "            markdown_text.append(\"#\"* heading_strength + ' ' + chunk['content'] + '\\n')\n",
    "        else:\n",
    "            markdown_text.append(f\"[{chunk['citation']}]\\n\" + chunk['content'] + f\"\\n[/{chunk['citation']}]\\n\")\n",
    "    return '\\n'.join(markdown_text)"
   ],
   "id": "736a743b59c6bb9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(format_chunks(final_cited_chunks[0]))",
   "id": "352112ea6877866f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def count_words_in_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Calculates the total number of words in a chunk of content.\n",
    "    A chunk is a list of dictionaries, where each dictionary has a 'content' key.\n",
    "\n",
    "    Args:\n",
    "        chunk (list): A list of content items (dictionaries).\n",
    "\n",
    "    Returns:\n",
    "        int: The total word count in the chunk.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for item in chunk:\n",
    "        # Split content by whitespace to count words\n",
    "        count += len(item.get('content', '').split())\n",
    "    return count\n",
    "\n",
    "def simple_chunker(data, target_word_count):\n",
    "    \"\"\"\n",
    "    Chunks a document by greedily adding passages until a target word count is reached.\n",
    "    This does not rely on any hierarchical structure.\n",
    "\n",
    "    Args:\n",
    "        data (list): The list of passages from the document.\n",
    "        target_word_count (int): The approximate word count for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunks.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for passage in data:\n",
    "        passage_word_count = count_words_in_chunk([passage])\n",
    "\n",
    "        # If the current chunk is not empty and adding the next passage would\n",
    "        # exceed the target, finalize the current chunk.\n",
    "        if current_chunk and (current_word_count + passage_word_count > target_word_count):\n",
    "            chunks.append(current_chunk)\n",
    "            # Start a new chunk\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "\n",
    "        # Add the passage to the current (or new) chunk.\n",
    "        current_chunk.append(passage)\n",
    "        current_word_count += passage_word_count\n",
    "\n",
    "    # Add the last remaining chunk if it exists.\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def add_citations(chunks, base_citation):\n",
    "    \"\"\"\n",
    "    Adds a unique citation and preserves the original document index for each passage.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): A list of chunks.\n",
    "        base_citation (str): The base string to use for citations.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of chunks with citations added to each passage.\n",
    "    \"\"\"\n",
    "    cited_chunks = []\n",
    "    # Enumerate from 1 to get human-readable chunk IDs\n",
    "    for chunk_id, chunk in enumerate(chunks, 1):\n",
    "        new_chunk = []\n",
    "        # Enumerate from 1 for passage IDs within the chunk\n",
    "        for passage_id, passage in enumerate(chunk, 1):\n",
    "            # Create a copy to preserve all existing fields, including 'doc_index'.\n",
    "            new_passage = passage.copy()\n",
    "            new_passage['citation'] = f\"{base_citation}_{chunk_id}__{passage_id}\"\n",
    "            new_chunk.append(new_passage)\n",
    "        cited_chunks.append(new_chunk)\n",
    "    return cited_chunks\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    JSON_FILE_PATH = 'test.json'\n",
    "    BASE_CITATION = \"LSB11249\" # Base citation for this specific document\n",
    "    TARGET_CHUNK_WORDS = 637\n",
    "\n",
    "    try:\n",
    "        # Load the document from the JSON file\n",
    "        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            document_data_raw = json.load(f)\n",
    "\n",
    "        # --- Step 1: Add original document index to each passage ---\n",
    "        document_data = []\n",
    "        for i, passage in enumerate(document_data_raw):\n",
    "            passage['doc_index'] = i\n",
    "            document_data.append(passage)\n",
    "        print(f\"Step 1: Added original document index to {len(document_data)} passages.\\n\")\n",
    "\n",
    "        # --- Step 2: Perform simple chunking based on word count ---\n",
    "        initial_chunks = simple_chunker(document_data, TARGET_CHUNK_WORDS)\n",
    "        print(\"---\" * 15)\n",
    "        print(f\"\\nStep 2: Document split into {len(initial_chunks)} chunks.\")\n",
    "        print(f\"Target Chunk Size: {TARGET_CHUNK_WORDS} words\\n\")\n",
    "\n",
    "        # --- Step 3: Add citations to the final chunks ---\n",
    "        final_cited_chunks = add_citations(initial_chunks, BASE_CITATION)\n",
    "        print(\"---\" * 15)\n",
    "        print(f\"\\nStep 3: Added citations to {len(final_cited_chunks)} chunks.\")\n",
    "        print(f\"Base Citation: '{BASE_CITATION}'\\n\")\n",
    "\n",
    "        # --- Output and Verification ---\n",
    "        print(\"---\" * 15)\n",
    "        print(\"\\nFinal Output Verification:\\n\")\n",
    "        total_word_count = 0\n",
    "        for i, chunk in enumerate(final_cited_chunks):\n",
    "            word_count = count_words_in_chunk(chunk)\n",
    "            total_word_count += word_count\n",
    "\n",
    "            first_item_type = chunk[0].get('type', 'N/A')\n",
    "            first_item_content = chunk[0].get('content', '').replace('\\n', ' ')[:70]\n",
    "            first_item_citation = chunk[0].get('citation', 'N/A')\n",
    "            first_item_doc_index = chunk[0].get('doc_index', -1)\n",
    "\n",
    "            print(f\"--- Final Chunk {i+1} ---\")\n",
    "            print(f\"  Word Count: {word_count}\")\n",
    "            print(f\"  Items: {len(chunk)}\")\n",
    "            print(f\"  Starts with '{first_item_type}': \\\"{first_item_content}...\\\"\")\n",
    "            print(f\"  First Citation: {first_item_citation}\")\n",
    "            print(f\"  First Doc Index: {first_item_doc_index}\")\n",
    "            print()\n",
    "\n",
    "        print(f\"Total word count across all final chunks: {total_word_count}\")\n",
    "        print(f\"Total word count in original document: {count_words_in_chunk(document_data)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{JSON_FILE_PATH}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from the file '{JSON_FILE_PATH}'.\")\n"
   ],
   "id": "1e1406d6d3609b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_cited_chunks[1]",
   "id": "de4e8dd09e324473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6d0403fe9850a391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from pathlib import Path",
   "id": "f4f5828326440e58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_folder = Path('reports/parsed')\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "errors_folder = Path('reports/errors')\n",
    "errors_folder.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "f55afadfc88dd792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "json_files = Path('reports/reports')\n",
    "json_files = [x for x in json_files.glob('*.json')]"
   ],
   "id": "4c335a091fe14c23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ],
   "id": "dbd3a709f0c8bd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_report_metadata(metadata):\n",
    "    parsed_metadata = dict()\n",
    "    parsed_metadata['id'] = metadata['id']\n",
    "    parsed_metadata['type'] = metadata['type']\n",
    "    parsed_metadata['typeId'] = metadata['typeId']\n",
    "    parsed_metadata['number'] = metadata['number']\n",
    "    parsed_metadata['active'] = metadata['active']\n",
    "    parsed_metadata['source'] = metadata['source']\n",
    "    parsed_metadata['topics'] = metadata['topics']\n",
    "    _version_info = metadata['versions'][0]\n",
    "    parsed_metadata['version_id'] = _version_info['id']\n",
    "    parsed_metadata['date'] = _version_info['date']\n",
    "    parsed_metadata['retrieved_date'] = _version_info['retrieved']\n",
    "    parsed_metadata['title'] = _version_info['title']\n",
    "    parsed_metadata['summary'] = _version_info['summary']\n",
    "    parsed_metadata['source_file'] = [x for x in _version_info['formats'] if x['format'] == 'HTML'][0]['filename']\n",
    "    return parsed_metadata"
   ],
   "id": "39c28b9cb0ff7986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parse_report_metadata(data)",
   "id": "766957857099c362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_file(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    try:\n",
    "        filename = [x for x in data['versions'][0]['formats'] if x['format'] == 'HTML'][0]['filename']\n",
    "    except IndexError as e:\n",
    "        return False, json_file.name, ''\n",
    "\n",
    "    with open(f'reports/{filename}', 'r') as f:\n",
    "        doc = f.read()\n",
    "    return True, doc, data"
   ],
   "id": "3846e5f8b772f4be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# BASE_CITATION = \"RL31572\"\n",
    "# Config for the initial splitting phase\n",
    "INITIAL_MAX_WORDS = 800\n",
    "INITIAL_BUFFER = 1300\n",
    "MINIMUM_NECESSARY_HEADINGS = 1\n",
    "# Config for the merging phase\n",
    "TARGET_MERGE_WORD_COUNT = 1500\n",
    "TARGET_CHUNK_WORDS = 1500"
   ],
   "id": "8e4e8511b90eaed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "errored_files = list()\n",
    "re_parse_files = list()\n",
    "parsed_data = list()\n",
    "total_chunks = 0\n",
    "total_parsed_chunks = 0"
   ],
   "id": "288c0bbc2e3d77e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_point = len(errored_files) + len(parse_report_metadata(data))\n",
    "for json_file in tqdm(json_files[start_point:]):\n",
    "    did_load, html_document, metadata = load_file(json_file)\n",
    "    if not did_load:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            _error_data = json.load(f)\n",
    "        errored_files.append({'filename': json_file.name, 'json':_error_data, 'error':'error_loading'})\n",
    "        continue\n",
    "\n",
    "    if len(html_document.strip()) == 0:\n",
    "        print('no html')\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            _error_data = json.load(f)\n",
    "        errored_files.append({'filename': json_file.name, 'json':_error_data, 'error':'no_html'})\n",
    "        continue\n",
    "\n",
    "    metadata = parse_report_metadata(metadata)\n",
    "    document_data_raw = parse_html_content(html_document)\n",
    "    document_data = []\n",
    "    for i, passage in enumerate(document_data_raw):\n",
    "        passage['doc_index'] = i\n",
    "        document_data.append(passage)\n",
    "    if len(document_data) == 0:\n",
    "        errored_files.append({'filename': json_file.name, 'json':metadata, 'error':'no_parsed_data'})\n",
    "        continue\n",
    "    # --- Step 1: Perform the initial hierarchical chunking ---\n",
    "    highest_heading = skip_heading_1(document_data, min_necessary=MINIMUM_NECESSARY_HEADINGS)\n",
    "    if highest_heading > 0:\n",
    "        initial_chunks = chunk_document(document_data, INITIAL_MAX_WORDS, INITIAL_BUFFER, target_level=highest_heading)\n",
    "    else:\n",
    "        initial_chunks = simple_chunker(document_data, TARGET_CHUNK_WORDS)\n",
    "\n",
    "    total_parsed_chunks += len(initial_chunks)\n",
    "    # initial_chunks = chunk_document(document_data, INITIAL_MAX_WORDS, INITIAL_BUFFER, target_level=highest_heading)\n",
    "    # print(f\"Step 1: Document initially split into {len(initial_chunks)} chunks.\\n\")\n",
    "\n",
    "    # --- Step 2: Perform the merging post-processing ---\n",
    "    final_merged_chunks = merge_chunks(initial_chunks, TARGET_MERGE_WORD_COUNT)\n",
    "    total_chunks += len(final_merged_chunks)\n",
    "    # print(\"---\" * 15)\n",
    "    # print(f\"\\nStep 2: Merged into {len(final_merged_chunks)} final chunks.\")\n",
    "    # print(f\"Target Merge Size: {TARGET_MERGE_WORD_COUNT} words\\n\")\n",
    "\n",
    "    # --- Step 3: Add citations to the final chunks ---\n",
    "    final_cited_chunks = add_citations(final_merged_chunks, metadata['id'])\n",
    "    metadata['chunks'] = final_cited_chunks\n",
    "    metadata['initial_chunks'] = initial_chunks\n",
    "    parsed_data.append(metadata)\n",
    "    # print(\"---\" * 15)\n",
    "    # print(f\"\\nStep 3: Added citations to {len(final_cited_chunks)} chunks.\")\n",
    "    # print(f\"Base Citation: '{BASE_CITATION}'\\n\")\n",
    "    #\n",
    "    # # --- Output and Verification ---\n",
    "    # print(\"---\" * 15)\n",
    "    # print(\"\\nFinal Output Verification:\\n\")\n",
    "    # total_word_count = 0\n",
    "    # for i, chunk in enumerate(final_cited_chunks):\n",
    "    #     word_count = count_words_in_chunk(chunk)\n",
    "    #     total_word_count += word_count\n",
    "    #\n",
    "    #     first_item_type = chunk[0].get('type', 'N/A')\n",
    "    #     first_item_content = chunk[0].get('content', '').replace('\\n', ' ')[:70]\n",
    "    #     first_item_citation = chunk[0].get('citation', 'N/A')\n",
    "    #     first_item_doc_index = chunk[0].get('doc_index', -1)\n",
    "    #\n",
    "    #     print(f\"--- Final Chunk {i+1} ---\")\n",
    "    #     print(f\"  Word Count: {word_count}\")\n",
    "    #     print(f\"  Items: {len(chunk)}\")\n",
    "    #     print(f\"  Starts with '{first_item_type}': \\\"{first_item_content}...\\\"\")\n",
    "    #     print(f\"  First Citation: {first_item_citation}\")\n",
    "    #     print(f\"  First Doc Index: {first_item_doc_index}\")\n",
    "    #     print()\n",
    "    #\n",
    "    # print(f\"Total word count across all final chunks: {total_word_count}\")\n",
    "    # print(f\"Total word count in original document: {count_words_in_chunk(document_data)}\")\n",
    "\n"
   ],
   "id": "dded6a8d3dc6c993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(parsed_data), len(errored_files), total_chunks, total_parsed_chunks",
   "id": "5aff957779660b04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "run_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "with open(save_folder.joinpath(f'{run_time}_parsed.json'), 'w') as f:\n",
    "    json.dump(parsed_data, f)\n",
    "with open(errors_folder.joinpath(f'{run_time}_errors.json'), 'w') as f:\n",
    "    json.dump(errored_files, f)"
   ],
   "id": "78e16b86d18d1c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parsed_data[-1]",
   "id": "ee8efe6d99937a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cfac06cca161c014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "903482f7672cd276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7a664b6882ba58b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4dab5a860baff959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "39d1d07adcc0fde9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class HierarchicalParser:\n",
    "    \"\"\"\n",
    "    A hierarchical parser to segment a document into roughly equivalent chunks\n",
    "    while respecting the document's structure and generating citations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, document, base_citation=\"doc\", max_chunk_size=500):\n",
    "        \"\"\"\n",
    "        Initializes the parser with the document, base citation, and chunk size.\n",
    "\n",
    "        Args:\n",
    "            document (list): A list of dictionaries, where each dictionary\n",
    "                             has a 'type' (str) and 'content' (str).\n",
    "            base_citation (str): The base string for generating citations.\n",
    "            max_chunk_size (int): The target maximum size for each chunk in words.\n",
    "        \"\"\"\n",
    "        self.document = document\n",
    "        self.base_citation = base_citation\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        # Defines the hierarchy of passage types from most to least important.\n",
    "        self.hierarchy = ['header_2', 'header_3', 'header_4', 'paragraph']\n",
    "        self.passage_type_map = {ptype: i for i, ptype in enumerate(self.hierarchy)}\n",
    "\n",
    "    def _get_passage_level(self, passage_type):\n",
    "        \"\"\"\n",
    "        Gets the hierarchical level of a passage type. Handles variations\n",
    "        like 'paragraph_markdown' by checking for substrings.\n",
    "        \"\"\"\n",
    "        if passage_type in self.passage_type_map:\n",
    "            return self.passage_type_map[passage_type]\n",
    "        for key, value in self.passage_type_map.items():\n",
    "            if key in passage_type:\n",
    "                return value\n",
    "        return len(self.hierarchy)\n",
    "\n",
    "    def _split_text(self, text, num_chunks):\n",
    "        \"\"\"Splits a single text into a specified number of smaller chunks based on word count.\"\"\"\n",
    "        if num_chunks <= 1:\n",
    "            return [text]\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return []\n",
    "        chunk_size = math.ceil(len(words) / num_chunks)\n",
    "        if chunk_size == 0:\n",
    "            return [text]\n",
    "        return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"\n",
    "        Parses the document and splits it into chunks with citations.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of chunks, where each chunk is a dictionary containing\n",
    "                  the 'content', 'hierarchy', and citation information.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        current_chunk_passages = []\n",
    "        current_chunk_size = 0\n",
    "        chunk_counter = 0\n",
    "\n",
    "        current_hierarchy_texts = []\n",
    "        current_hierarchy_levels = []\n",
    "\n",
    "        def finalize_chunk(passages, hierarchy):\n",
    "            \"\"\"Helper function to format and add a chunk to the main list.\"\"\"\n",
    "            nonlocal chunk_counter\n",
    "            if not passages:\n",
    "                return\n",
    "\n",
    "            chunk_counter += 1\n",
    "            content_with_citations = []\n",
    "            for i, passage in enumerate(passages):\n",
    "                citation = f\"[{self.base_citation}_{chunk_counter}__{i+1}]\"\n",
    "                content_with_citations.append({\n",
    "                    'type': passage['type'],\n",
    "                    'content': passage['content'],\n",
    "                    'citation': citation\n",
    "                })\n",
    "\n",
    "            chunks.append({\n",
    "                'hierarchy': list(hierarchy),\n",
    "                'content': content_with_citations\n",
    "            })\n",
    "\n",
    "        for i, passage in enumerate(self.document):\n",
    "            passage_type = passage['type']\n",
    "            text = passage['content']\n",
    "            passage_size = len(text.split())\n",
    "            passage_level = self._get_passage_level(passage_type)\n",
    "\n",
    "            # --- Special handling for single passages larger than the chunk size ---\n",
    "            if passage_size > self.max_chunk_size:\n",
    "                finalize_chunk(current_chunk_passages, current_hierarchy_texts)\n",
    "                current_chunk_passages, current_chunk_size = [], 0\n",
    "\n",
    "                if 'header' in passage_type:\n",
    "                    while current_hierarchy_levels and current_hierarchy_levels[-1] >= passage_level:\n",
    "                        current_hierarchy_levels.pop()\n",
    "                        current_hierarchy_texts.pop()\n",
    "                    current_hierarchy_levels.append(passage_level)\n",
    "                    current_hierarchy_texts.append(text)\n",
    "\n",
    "                num_sub_chunks = math.ceil(passage_size / self.max_chunk_size)\n",
    "                split_texts = self._split_text(text, num_sub_chunks)\n",
    "                for sub_text in split_texts:\n",
    "                    finalize_chunk([{'type': passage_type, 'content': sub_text}], current_hierarchy_texts)\n",
    "                continue\n",
    "\n",
    "            # --- Simplified logic to decide when to finalize a chunk ---\n",
    "            start_new_chunk = False\n",
    "            if current_chunk_passages:\n",
    "                # Condition 1: New header at same or higher level creates a new chunk.\n",
    "                last_header_level = current_hierarchy_levels[-1] if current_hierarchy_levels else len(self.hierarchy)\n",
    "                is_new_section = 'header' in passage_type and passage_level <= last_header_level\n",
    "\n",
    "                # Condition 2: Chunk size overflow.\n",
    "                exceeds_size = (current_chunk_size + passage_size) > self.max_chunk_size\n",
    "\n",
    "                if is_new_section or exceeds_size:\n",
    "                    start_new_chunk = True\n",
    "\n",
    "            if start_new_chunk:\n",
    "                passages_to_finalize = list(current_chunk_passages)\n",
    "                carry_over_passages = []\n",
    "\n",
    "                # Prevent chunk from ending on a header by carrying them to the next chunk.\n",
    "                while passages_to_finalize and 'header' in passages_to_finalize[-1]['type']:\n",
    "                    carry_over_passages.insert(0, passages_to_finalize.pop())\n",
    "\n",
    "                # Determine the hierarchy for the chunk being finalized.\n",
    "                hierarchy_for_finalized_chunk = list(current_hierarchy_texts)\n",
    "                if carry_over_passages:\n",
    "                    num_carried = len(carry_over_passages)\n",
    "                    hierarchy_for_finalized_chunk = hierarchy_for_finalized_chunk[:-num_carried]\n",
    "\n",
    "                finalize_chunk(passages_to_finalize, hierarchy_for_finalized_chunk)\n",
    "\n",
    "                # The carried-over passages start the new chunk.\n",
    "                current_chunk_passages = carry_over_passages\n",
    "                current_chunk_size = sum(len(p['content'].split()) for p in current_chunk_passages)\n",
    "\n",
    "            # Add the current passage to the chunk and update hierarchy\n",
    "            if 'header' in passage_type:\n",
    "                while current_hierarchy_levels and current_hierarchy_levels[-1] >= passage_level:\n",
    "                    current_hierarchy_levels.pop()\n",
    "                    current_hierarchy_texts.pop()\n",
    "                current_hierarchy_levels.append(passage_level)\n",
    "                current_hierarchy_texts.append(text)\n",
    "\n",
    "            current_chunk_passages.append(passage)\n",
    "            current_chunk_size += passage_size\n",
    "\n",
    "        # Finalize any remaining passages in the last chunk.\n",
    "        finalize_chunk(current_chunk_passages, current_hierarchy_texts)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # A sample document to test section changes and splitting.\n",
    "    sample_document = [\n",
    "        {'type': 'header_2', 'content': 'Section 1: The First Topic'},\n",
    "        {'type': 'paragraph', 'content': 'This is content within the first section. It introduces the main ideas.'},\n",
    "        {'type': 'header_3', 'content': 'Subsection 1.1'},\n",
    "        {'type': 'paragraph', 'content': 'Content for subsection 1.1. This provides more detail.'},\n",
    "        {'type': 'header_3', 'content': 'Subsection 1.2'},\n",
    "        {'type': 'paragraph', 'content': 'Content for subsection 1.2. A new chunk should start here because we have a new header at the same level as the previous one.'},\n",
    "        {'type': 'header_2', 'content': 'Section 2: A Higher-Level Change'},\n",
    "        {'type': 'paragraph', 'content': 'This content belongs to Section 2. A new chunk should have been created for this section because its header is a higher level than \"Subsection 1.2\".'},\n",
    "        {'type': 'paragraph', 'content': 'This is a very long paragraph designed to test the chunking mechanism when a single passage might be larger than the chunk size or push the current chunk over the limit. It continues to describe the topic in great detail, forcing a split based on size rather than on a header change. The hierarchy should be preserved across the split.'},\n",
    "        {'type': 'header_3', 'content': 'Subsection 2.1'}\n",
    "    ]\n",
    "\n",
    "    # Initialize the parser with a small chunk size to demonstrate the logic.\n",
    "    parser = HierarchicalParser(sample_document, base_citation=\"TestDoc\", max_chunk_size=75)\n",
    "\n",
    "    # Get the chunks\n",
    "    document_chunks = parser.parse()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Document was split into {len(document_chunks)} chunks.\\n\")\n",
    "    for i, chunk_data in enumerate(document_chunks):\n",
    "        content = chunk_data['content']\n",
    "        hierarchy = chunk_data['hierarchy']\n",
    "        chunk_size = sum(len(passage['content'].split()) for passage in content)\n",
    "\n",
    "        print(f\"--- Chunk {i+1} (Size: {chunk_size} words) ---\")\n",
    "        print(f\"  Hierarchy: {hierarchy}\")\n",
    "        for passage in content:\n",
    "            print(f\"    {passage['citation']} [{passage['type']}] {passage['content'][:80]}...\")\n",
    "        print(\"\\n\")\n"
   ],
   "id": "253691c6c9a78cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "document_chunks[3]",
   "id": "e6fd1bcd86a2907",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aa3833ae35951495",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
