{
 "cells": [
  {
   "cell_type": "code",
   "id": "a818b796-83d6-4dd2-ba79-0a71e8b76a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T01:40:15.727192Z",
     "start_time": "2025-04-13T01:40:15.328835Z"
    }
   },
   "source": [
    "import pypandoc\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pysbd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d724e131-8d34-4d5d-a076-8d262190a195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T01:40:15.736702Z",
     "start_time": "2025-04-13T01:40:15.729565Z"
    }
   },
   "source": [
    "def extract_divs(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    divs = soup.select('div')\n",
    "    return divs\n",
    "\n",
    "def clean_div(div):\n",
    "    html_div = str(div)\n",
    "    # replace_empty_italic = re.compile(r'</?div.*?>')\n",
    "    # html_div = replace_empty_italic.sub('', html_div)\n",
    "    html_div = re.sub(r'<sup>(.*?)</sup>', r' [\\1]', html_div)\n",
    "    html_div = html_div.strip()\n",
    "    return html_div\n",
    "\n",
    "def convert_to_markdown_sections(html_div):\n",
    "    markdown_conversion = pypandoc.convert_text(html_div, 'gfm' ,'html')\n",
    "    markdown_conversion = markdown_conversion.replace('<div>\\n','')\n",
    "    markdown_conversion = re.sub(r'</?div>[\\s\\n]*', '', markdown_conversion)\n",
    "    return markdown_conversion\n",
    "\n",
    "def clean_sections(markdown_conversion):\n",
    "    markdown_sections_with_delimiters = re.split(r'(##)', markdown_conversion)\n",
    "    markdown_sections_with_delimiters = [s for s in markdown_sections_with_delimiters if s]\n",
    "    combined_sections = []\n",
    "    current_section = \"\"\n",
    "    for item in markdown_sections_with_delimiters:\n",
    "        if item == '##' or item == \"#\":\n",
    "            if current_section: # Add the previous section if it exists\n",
    "                combined_sections.append(current_section.strip())\n",
    "            current_section = item # Start the new section with the delimiter\n",
    "        else:\n",
    "            current_section += item\n",
    "    if current_section: # Add the last section\n",
    "        combined_sections.append(current_section.strip())\n",
    "    return combined_sections\n",
    "\n",
    "def condense_paragraphs(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence[-2:] == '\\n\\n' and sentence[0] in ['*','#']:\n",
    "            cleaned_sentences.append(sentence[:-1])\n",
    "        elif sentence.strip()[0] == '-':\n",
    "            cleaned_sentences.append('\\n' + sentence)\n",
    "        elif sentence[-2:] == '\\n\\n':\n",
    "            cleaned_sentences.append(sentence[:-1])\n",
    "        elif sentence[-1:] == '\\n':\n",
    "            cleaned_sentences.append(sentence[:-1])\n",
    "        elif sentence.strip(' ')[-1:] == '\\n':\n",
    "            cleaned_sentences.append(sentence.strip())\n",
    "        else:\n",
    "            cleaned_sentences.append(sentence)\n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "def label_section(sections):\n",
    "    labeled_sections = []\n",
    "    for section in sections:\n",
    "        section_type = \"section\"\n",
    "        if '#_Toc' in section:\n",
    "            section_type = 'table_of_contents'\n",
    "        elif '---------' in section:\n",
    "            section_type = 'header'\n",
    "        labeled_sections.append({'text': section, 'type': section_type})\n",
    "    return labeled_sections\n",
    "\n",
    "def clean_passage(sentences):\n",
    "    cleaned_passage = condense_paragraphs(sentences)\n",
    "    cleaned_passage = re.sub(r\"\\\\\\[<a.*?>(\\w*)<.*?\\]\", r'[\\1]', cleaned_passage)\n",
    "    cleaned_passage = re.sub(r\"<span.*?>.*?</span>\",\"\", cleaned_passage, flags=re.DOTALL)\n",
    "    cleaned_passage = cleaned_passage.strip()\n",
    "    cleaned_passage = re.sub(r'\\[\\]\\(#_.*?\\)', '', cleaned_passage)\n",
    "    cleaned_passage = re.sub(r'\\[.*?Table.*?\\]\\(#_.*?\\)', '', cleaned_passage)\n",
    "    cleaned_passage = re.sub(r'\\[.*?Figure.*?\\]\\(#_.*?\\)', '', cleaned_passage)\n",
    "    cleaned_passage = re.sub(r'\\*{4,}', '', cleaned_passage)\n",
    "    cleaned_passage = re.sub(r'<div.*?>', '', cleaned_passage)\n",
    "    return cleaned_passage\n",
    "\n",
    "def process_div(div):\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    div = clean_div(div)\n",
    "    markdown_conversion = convert_to_markdown_sections(div)\n",
    "    sections = clean_sections(markdown_conversion)\n",
    "    cleaned_passages = list()\n",
    "    for section in sections:\n",
    "        sentences = seg.segment(section)\n",
    "        cleaned_passages.append(clean_passage(sentences))\n",
    "    return cleaned_passages\n",
    "\n",
    "def extract_unique_segments(html_segments):\n",
    "    html_segments = [segment for segment in html_segments if segment.get_text().strip() != '']\n",
    "    unique_segments = list()\n",
    "    unique_tracker = list()\n",
    "    for segment in html_segments:\n",
    "        if segment.get_text().strip() not in unique_tracker:\n",
    "            unique_segments.append(segment)\n",
    "            unique_tracker.append(segment.get_text().strip())\n",
    "    if unique_segments[-1].get_text().strip() == 'Footnotes':\n",
    "        unique_segments = unique_segments[:-1]\n",
    "    return unique_segments\n",
    "\n",
    "def process_div_segments_to_text(unique_segments):\n",
    "    processed_sections = process_div(unique_segments[0])\n",
    "    processed_sections = [x.strip() for x in processed_sections if not re.match(r'^\\#+$', x.strip())]\n",
    "    return processed_sections\n",
    "\n",
    "def extract_json_information(report_json):\n",
    "    overview_fields = ['id','type','typeId','number','active','topics']\n",
    "    document_fields = ['date','title','summary']\n",
    "\n",
    "    document = dict()\n",
    "    for _field in overview_fields:\n",
    "        document[_field] = report_json[_field]\n",
    "\n",
    "    sorted_metadata = sorted(report_json['versions'], key=lambda x: x['date'], reverse=True)[0]\n",
    "\n",
    "    # for report_file in sorted_metadata['formats']:\n",
    "    #     if report_file['format'] == 'HTML':\n",
    "    #         file_name = report_file[\"filename\"]\n",
    "\n",
    "    file_name = [x.name for x in html_folder.glob('*.html') if report_json['id'] in x.name]\n",
    "    if len(file_name) > 0:\n",
    "        file_name = file_name[0]\n",
    "    else:\n",
    "        file_name = None\n",
    "\n",
    "    for _field in document_fields:\n",
    "        document[_field] = sorted_metadata[_field]\n",
    "    document['doc_id'] = sorted_metadata['id']\n",
    "\n",
    "    document['filename'] = file_name\n",
    "    return document\n",
    "\n",
    "def process_html_file(filename, min_len=30):\n",
    "    with open(html_folder.joinpath(filename),'r') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    html = re.sub('<table>(.*?)</table>', '<div>', html, flags=re.MULTILINE | re.DOTALL)\n",
    "    html = re.sub('(</?img.+?>)', '', html, flags=re.MULTILINE | re.DOTALL)\n",
    "    return html\n",
    "\n",
    "def process_aname(div):\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    div = clean_div(div)\n",
    "    markdown_conversion = convert_to_markdown_sections(div)\n",
    "    sections = clean_sections(markdown_conversion)\n",
    "    cleaned_passages = list()\n",
    "    for section in sections:\n",
    "        sentences = seg.segment(section)\n",
    "        cleaned_passages.append(clean_passage(sentences))\n",
    "\n",
    "    cleaned_passages = [x.strip() for x in cleaned_passages if not re.match(r'^\\#+$', x.strip())]\n",
    "    return cleaned_passages[0]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "db9833fa-2559-4af4-9592-bc0e7d2396da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:51:51.535811Z",
     "start_time": "2025-04-13T11:51:49.571812Z"
    }
   },
   "source": [
    "report_folder = Path('../wonky_data/full_reports')\n",
    "json_folder = report_folder.joinpath('reports')\n",
    "html_folder = Path('../wonky_data/files')\n",
    "\n",
    "save_folder = Path('../wonky_data/parsed_reports')\n",
    "whole_folder = save_folder.joinpath('whole_doc')\n",
    "section_folder = save_folder.joinpath('sections')\n",
    "whole_folder.mkdir(parents=True, exist_ok=True)\n",
    "section_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "existing_files = [x.name for x in list(section_folder.glob('*.json'))]\n",
    "\n",
    "report_files = [x for x in list(json_folder.glob('*.json'))]\n",
    "\n",
    "new_files = [x for x in report_files if x.name not in existing_files]"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:51:51.548039Z",
     "start_time": "2025-04-13T11:51:51.545860Z"
    }
   },
   "cell_type": "code",
   "source": "len(report_files)",
   "id": "d4aa64fe6c1dfb42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22195"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "a41bec72-ebf3-4ef0-8779-1954c42ea6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:51:51.624240Z",
     "start_time": "2025-04-13T11:51:51.622258Z"
    }
   },
   "source": [
    "len(existing_files)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21514"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "d8f72e53-bf85-48da-bd4a-36ea3f595c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:51:51.698427Z",
     "start_time": "2025-04-13T11:51:51.696416Z"
    }
   },
   "source": [
    "len(new_files)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "ce9fff20-b68f-4163-9482-fc09078d639a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:51:52.522686Z",
     "start_time": "2025-04-13T11:51:52.520237Z"
    }
   },
   "source": "error_count = 0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "b17ab29c-e288-4cc8-b40f-ed7166bc737c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.414761Z",
     "start_time": "2025-04-13T11:51:53.031885Z"
    }
   },
   "source": [
    "sources = list()\n",
    "for source_file in tqdm(sorted(new_files)):\n",
    "    with open(source_file,'r') as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    extracted_report = extract_json_information(report)\n",
    "    if extracted_report['filename'] is None:\n",
    "        continue\n",
    "        \n",
    "    extracted_report['source_file'] = source_file.name\n",
    "    \n",
    "    html = process_html_file(extracted_report['filename'])\n",
    "    if len(html.strip()) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        if re.search(r'<a name=\\d+>', html):\n",
    "            html = html.replace('<br/>','\\n')\n",
    "            html_segments = [BeautifulSoup(x, 'html.parser') for x in html.split('<hr/>')]\n",
    "            text_segments = extract_unique_segments(html_segments)\n",
    "            text_segments = [process_aname(segment) for segment in text_segments]\n",
    "        else:\n",
    "            html_segments = extract_divs(html)\n",
    "            html_segments = extract_unique_segments(html_segments)\n",
    "            text_segments = process_div_segments_to_text(html_segments)\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        continue\n",
    "    full_text = '\\n\\n'.join(text_segments)\n",
    "    _number_of_sections = len(text_segments)\n",
    "    numbered_sections = {i:segment for i,segment in enumerate(text_segments, start=1)}\n",
    "    metadata = deepcopy(extracted_report)\n",
    "    metadata['text'] = full_text\n",
    "    metadata['sections'] = numbered_sections\n",
    "    if len(metadata['text'].strip()) == 0:\n",
    "        continue\n",
    "    with open(section_folder.joinpath(f\"\"\"{source_file.stem}.json\"\"\"),'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    # break\n",
    "    # full_doc_data['document'] = sections_doc\n",
    "    # sources.append(full_doc_data)\n",
    "    # with open(whole_folder.joinpath(source_file.name),'w') as f:\n",
    "    #     json.dump(full_doc_data, f)\n",
    "    \n",
    "        \n",
    "    # sections_data = list()\n",
    "    # for i, section in enumerate(sections_doc):\n",
    "    #     section_data = deepcopy(extracted_report)\n",
    "    #     section_data['page'] = i\n",
    "    #     section_data['page_id'] = f\"{section_data['id']}_{i:03}\"\n",
    "    #     section_data['document'] = section\n",
    "    #     sections_data.append(section_data)\n",
    "    #     with open(section_folder.joinpath(f\"\"\"{source_file.stem}_{i:03}.json\"\"\"),'w') as f:\n",
    "    #         json.dump(full_doc_data, f)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4843f196ea1b49b5912130f87af1b101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "291f3a06-0efc-40b8-8522-8b29e3168c15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:33:08.784923Z",
     "start_time": "2025-04-12T22:33:08.781358Z"
    }
   },
   "source": "type(text_segments[0])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:33:09.202570Z",
     "start_time": "2025-04-12T22:33:09.200245Z"
    }
   },
   "cell_type": "code",
   "source": "html",
   "id": "6fd9111c63fc73c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<a name=1></a>\\n\\n<hr/>\\n<a name=2></a>\\n\\n<hr/>\\n<a name=3></a>\\n\\n<hr/>\\n<a name=4></a>\\n\\n<hr/>\\n<a name=5></a>\\n\\n<hr/>\\n<a name=6></a>\\n\\n<hr/>\\n<a name=7></a>\\n\\n<hr/>\\n<a name=8></a>\\n\\n<hr/>\\n<a name=9></a>\\n\\n<hr/>\\n<a name=10></a>\\n\\n<hr/>\\n<a name=11></a>\\n\\n<hr/>\\n<a name=12></a>\\n\\n<hr/>\\n<a name=13></a>\\n\\n<hr/>\\n<a name=14></a>\\n\\n<hr/>\\n<a name=15></a>\\n\\n<hr/>\\n<a name=16></a>\\n\\n<hr/>\\n<a name=17></a>\\n\\n<hr/>\\n<a name=18></a>\\n\\n<hr/>\\n<a name=19></a>\\n\\n<hr/>\\n<a name=20></a>\\n\\n<hr/>\\n<a name=21></a>\\n\\n<hr/>\\n<a name=22></a>\\n\\n<hr/>\\n<a name=23></a>\\n\\n<hr/>\\n<a name=24></a>\\n\\n<hr/>\\n<a name=25></a>\\n\\n<hr/>\\n<a name=26></a>\\n\\n<hr/>\\n<a name=27></a>\\n\\n<hr/>\\n<a name=28></a>\\n\\n<hr/>\\n<a name=29></a>\\n\\n<hr/>\\n<a name=30></a>\\n\\n<hr/>\\n<a name=31></a>\\n\\n<hr/>\\n<a name=32></a>\\n\\n<hr/>\\n<a name=33></a>\\n\\n<hr/>\\n<a name=34></a>\\n\\n<hr/>\\n<a name=35></a>\\n\\n<hr/>\\n<a name=36></a>\\n\\n<hr/>\\n<a name=37></a>\\n\\n<hr/>\\n<a name=38></a>\\n\\n<hr/>\\n<a name=39></a>\\n\\n<hr/>\\n<a name=40></a>\\n\\n<hr/>\\n<a name=41></a>\\n\\n<hr/>\\n<a name=42></a>\\n\\n<hr/>\\n<a name=43></a>\\n\\n<hr/>\\n<a name=44></a>\\n\\n<hr/>\\n<a name=45></a>\\n\\n<hr/>\\n<a name=46></a>\\n\\n<hr/>\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "id": "f936c3bf-655f-498a-8650-c458d326a873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:33:13.632750Z",
     "start_time": "2025-04-12T22:33:13.629271Z"
    }
   },
   "source": [
    "with open('test_html.html','w') as f:\n",
    "    f.write(html)"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:33:14.118884Z",
     "start_time": "2025-04-12T22:33:14.115523Z"
    }
   },
   "cell_type": "code",
   "source": "re.search(r'<a name=\\d+', html)",
   "id": "4b273f9cb7e3364e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 10), match='<a name=1'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:33:14.777223Z",
     "start_time": "2025-04-12T22:33:14.774159Z"
    }
   },
   "cell_type": "code",
   "source": "'<a name' in html",
   "id": "63a8e36a8d321ec7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:33:38.400948Z",
     "start_time": "2025-04-12T22:33:38.398715Z"
    }
   },
   "cell_type": "code",
   "source": "extracted_report['filename']",
   "id": "cd3d01946b610355",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19840302_84-29S_071a5bb73087555a529778e4b292f7c980c586fe.html'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:34:06.110889Z",
     "start_time": "2025-04-12T22:34:06.104562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(html_folder.joinpath(extracted_report['filename']), 'r') as f:\n",
    "    html = f.read()"
   ],
   "id": "18fa92e2df3869f7",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T22:34:06.422237Z",
     "start_time": "2025-04-12T22:34:06.418255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('test_html.html','w') as f:\n",
    "    f.write(html)"
   ],
   "id": "5d2747800c1a4fd6",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d168f9011f711953"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
