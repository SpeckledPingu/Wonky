{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:38:41.536482Z",
     "start_time": "2025-04-14T13:38:41.534417Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import lancedb\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7646b6bdd9e7ac42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:10:32.094802Z",
     "start_time": "2025-04-14T13:10:32.092265Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1119cfe375fe3f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:09:32.948436Z",
     "start_time": "2025-04-14T13:09:30.940847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', device='mps',trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337f925aabddfab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:09:32.981984Z",
     "start_time": "2025-04-14T13:09:32.955305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'type', 'typeId', 'number', 'active', 'topics', 'date', 'title', 'summary', 'doc_id', 'filename', 'source_file', 'text', 'vector', 'section_id', '_score'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_folder = Path('../wonky_data/indexes')\n",
    "index = lancedb.connect(index_folder)\n",
    "tbl = index.open_table('sections_fts')\n",
    "result = tbl.search('africa').limit(1).to_list()[0]\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b977602200df9dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:09:33.044881Z",
     "start_time": "2025-04-14T13:09:33.043294Z"
    }
   },
   "outputs": [],
   "source": [
    "source_files = Path('../wonky_data/parsed_reports/sections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee34839625ac040d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:09:34.490470Z",
     "start_time": "2025-04-14T13:09:34.375447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../wonky_data/parsed_reports/sections/96-404.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(source_files.glob('*.*'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48eb57357e0453a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:09:40.823491Z",
     "start_time": "2025-04-14T13:09:40.820949Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1da168d25d6432b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T13:10:43.969058Z",
     "start_time": "2025-04-14T13:10:38.427112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ddde76bd764699b89a282943f86f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = source_files.glob('*.json')\n",
    "data_files = list(data_files)\n",
    "documents = list()\n",
    "for file in tqdm(data_files):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        documents.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6dce48739c80992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:39:38.654122Z",
     "start_time": "2025-04-14T14:39:38.647345Z"
    }
   },
   "outputs": [],
   "source": [
    "import math # Used for ceiling division if needed, though simple len(split()) is fine here\n",
    "\n",
    "def combine_sections_by_word_count(sections, report_id, max_word_count):\n",
    "    \"\"\"\n",
    "    Combines text sections into chunks, ensuring each chunk does not exceed\n",
    "    the specified maximum word count.\n",
    "\n",
    "    Args:\n",
    "        sections (list): A list of strings, where each string is a text section.\n",
    "        max_word_count (int): The maximum number of words allowed in a chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is a combined chunk.\n",
    "    \"\"\"\n",
    "    combined_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_word_count = 0\n",
    "    current_sections = list()\n",
    "    combined_sections = list()\n",
    "    combined_subsection_text = list()\n",
    "    current_subsection_text = list()\n",
    "    for section_id, section in sections.items():\n",
    "        # Basic word count by splitting on space\n",
    "        # Note: This is a simple approach and might not perfectly handle\n",
    "        # all punctuation or multiple spaces.\n",
    "        section_words = section.split(' ')\n",
    "        # Filter out empty strings that can result from multiple spaces\n",
    "        section_words = [word for word in section_words if word]\n",
    "        section_word_count = len(section_words)\n",
    "\n",
    "        # If the section itself is larger than the max count, add it as its own chunk\n",
    "        if section_word_count > max_word_count and current_word_count == 0:\n",
    "            # print(f\"Warning: Section starting with '{section[:50]}...' exceeds max_word_count ({section_word_count} > {max_word_count}) and will be added as its own chunk.\")\n",
    "            combined_chunks.append(section + f\" ({report_id}({section_id}))\")\n",
    "            combined_sections.append([int(section_id)])\n",
    "            # Reset for the next potential chunk (although this chunk is done)\n",
    "            current_chunk = \"\"\n",
    "            current_word_count = 0\n",
    "            current_sections = list()\n",
    "            current_subsection_text = list()\n",
    "            # print(current_sections)\n",
    "            continue # Move to the next section\n",
    "\n",
    "        # Check if adding the new section would exceed the limit\n",
    "        if current_word_count + section_word_count <= max_word_count:\n",
    "            # Add section to the current chunk\n",
    "            if current_chunk: # Add a space if the chunk isn't empty\n",
    "                current_chunk += \"\\n\" + section + f\" ({report_id}({section_id}))\"\n",
    "                current_sections.append(int(section_id))\n",
    "                current_subsection_text.append({int(section_id):section})\n",
    "            else:\n",
    "                current_chunk = section + f\" ({report_id}({section_id}))\"\n",
    "                current_sections.append(int(section_id))\n",
    "                current_subsection_text.append({int(section_id):section})\n",
    "            current_word_count += section_word_count\n",
    "        else:\n",
    "            # Current chunk is full, add it to the list\n",
    "            if current_chunk:\n",
    "                combined_chunks.append(current_chunk)\n",
    "                combined_sections.append(current_sections)\n",
    "                current_sections = list()\n",
    "                current_chunk = \"\"\n",
    "                # current_subsection_text = list()\n",
    "\n",
    "            # Start a new chunk with the current section\n",
    "            # Check again if this new section *alone* exceeds the limit\n",
    "            if section_word_count <= max_word_count:\n",
    "                current_chunk = section + f\" ({report_id}({section_id}))\"\n",
    "                current_sections.append(int(section_id))\n",
    "                current_subsection_text.append({int(section_id):section})\n",
    "                current_word_count = section_word_count\n",
    "            else:\n",
    "                # This section is too large even on its own\n",
    "                # print(f\"Warning: Section starting with '{section[:50]}...' exceeds max_word_count ({section_word_count} > {max_word_count}) and will be added as its own chunk.\")\n",
    "                combined_chunks.append(section + f\" ({report_id}({section_id}))\")\n",
    "                current_sections.append(int(section_id))\n",
    "                current_subsection_text.append({int(section_id):section})\n",
    "                combined_sections.append(current_sections)\n",
    "                # Reset for the next potential chunk\n",
    "                current_chunk = \"\"\n",
    "                current_word_count = 0\n",
    "                current_sections = list()\n",
    "                current_subsection_text = list()\n",
    "                # print(current_sections)\n",
    "\n",
    "\n",
    "    # Add the last remaining chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        combined_chunks.append(current_chunk)\n",
    "        combined_sections.append(current_sections)\n",
    "\n",
    "    return combined_chunks, combined_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10ae7cf067b278",
   "metadata": {},
   "source": [
    "for document in tqdm(documents):\n",
    "    combined_text, combined_sections = combine_sections_by_word_count(document['sections'], document['id'], 2500)\n",
    "    combined_vectors = encoder.encode(combined_text, padding=False, batch_size=3, show_progress_bar=True).tolist()\n",
    "    torch.mps.empty_cache()\n",
    "    for section_text, section_ids, vector in zip(combined_text, combined_sections, combined_vectors):\n",
    "        section_data = deepcopy(document)\n",
    "        section_data['section_ids'] = section_ids\n",
    "        section_data['text'] = section_text\n",
    "        section_data['section_start'] = section_ids[0] if isinstance(section_ids, list) else section_ids\n",
    "        section_data['section_end'] = section_ids[-1] if isinstance(section_ids, list) else section_ids\n",
    "        section_data['vector'] = vector\n",
    "        split_sections.append(section_data)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6531f3ac701b4",
   "metadata": {},
   "source": [
    "section_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e0fddb42f5749",
   "metadata": {},
   "source": [
    "section_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30e9ab860f129a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:42:16.455120Z",
     "start_time": "2025-04-14T14:42:16.451086Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_sections(documents, max_word_count=2500, batch_size=3):\n",
    "    all_sections = list()\n",
    "    for document in tqdm(documents):\n",
    "        combined_text, combined_sections = combine_sections_by_word_count(document['sections'], document['id'], max_word_count)\n",
    "        combined_vectors = encoder.encode(combined_text, padding=False, batch_size=batch_size, show_progress_bar=False).tolist()\n",
    "        torch.mps.empty_cache()\n",
    "        split_sections = list()\n",
    "        for section_text, section_ids, vector in zip(combined_text, combined_sections, combined_vectors):\n",
    "            section_data = deepcopy(document)\n",
    "            section_data['section_ids'] = section_ids\n",
    "            section_data['text'] = section_text\n",
    "            section_data['section_start'] = section_ids[0] if isinstance(section_ids, list) else section_ids\n",
    "            section_data['section_end'] = section_ids[-1] if isinstance(section_ids, list) else section_ids\n",
    "            section_data['vector'] = vector\n",
    "            section_data['sections'] = {int(subsection_id):subsection_text for subsection_id, subsection_text in document['sections'].items() if int(subsection_id) in section_ids}\n",
    "            split_sections.append(section_data)\n",
    "        all_sections.extend(split_sections)\n",
    "    return all_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883cb646e1fd13f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:42:16.596484Z",
     "start_time": "2025-04-14T14:42:16.594117Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "vector_batch_size = 3\n",
    "max_word_count = 1000\n",
    "save_folder = Path('../wonky_data/index_data/sections')\n",
    "save_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d0f897a96032f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:44:24.568918Z",
     "start_time": "2025-04-14T14:42:22.255183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574a926cc0824397b4fc99263c780512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128f71bbab8b42a79e7b2545d0ff3ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a9f7191ceb434b86334356c822f3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217201174d8a4f6a831028295ee77682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1586fe89ed54ee495db61c908881e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48289d279d834769993f83b836c1de05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for start_idx in tqdm(range(800, len(documents), batch_size)):\n",
    "    batch_data = documents[start_idx:start_idx+batch_size]\n",
    "    batch_sections = vectorize_sections(batch_data, max_word_count=max_word_count, batch_size=vector_batch_size)\n",
    "    with gzip.open(save_folder.joinpath(f\"vectorized_sections_{start_idx:04}_{start_idx + batch_size:04}.json.gz\"), 'wt') as f:\n",
    "        json.dump(batch_sections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "145022aeca193015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T14:40:48.957970Z",
     "start_time": "2025-04-14T14:40:48.955815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1761"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf0941-f306-413d-9336-32fda1a6e9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
